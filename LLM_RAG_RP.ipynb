{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0F3KTZ433eT",
        "outputId": "13f912b8-73bb-4bb9-9026-ab2fec91ca74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "#@title setup packages\n",
        "\n",
        "\n",
        "# Import necessary modules\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOa6ST1gSHAp"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOiiLzHZSHL_",
        "outputId": "c2b7a8bf-3834-402c-ff71-e1716cf5f5dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43131"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading and storing datasets\n",
        "# Load documents (e.g., from local storage)\n",
        "#document_loader = DocumentLoader(\"path_to_local_storage\")\n",
        "\n",
        "#Loading from web\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "len(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcG4VN92SbZi"
      },
      "source": [
        "# Splitting dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ea52IZJSg1E",
        "outputId": "04bb45dc-3029-4c07-8ba1-3662bb5fe0ff"
      },
      "outputs": [],
      "source": [
        "#Define different text splitter methods\n",
        "# Recursive Character Text Splitter\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from  langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "\n",
        "all_docs = []\n",
        "all_splits = []\n",
        "\n",
        "pdf_directory = \"./\"\n",
        "\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000, add_start_index=True)\n",
        "\n",
        "for filename in os.listdir(pdf_directory):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdfloader = PyPDFLoader(os.path.join(pdf_directory,filename))\n",
        "        docs = pdfloader.load_and_split(text_splitter=recursive_splitter)\n",
        "        \n",
        "        for doc in docs:\n",
        "            all_docs.append(doc)\n",
        "        \n",
        "\n",
        "\n",
        "# HTML Header Text Splitter\n",
        "#html_splitter = HTMLHeaderTextSplitter()\n",
        "\n",
        "# Markdown Header Text Splitter\n",
        "#markdown_splitter = MarkdownHeaderTextSplitter()\n",
        "\n",
        "# Code Text Splitter\n",
        "#code_splitter = CodeTextSplitter()\n",
        "\n",
        "# Token Text Splitter\n",
        "#token_splitter = TokenTextSplitter()\n",
        "\n",
        "# Character Text Splitter\n",
        "#character_splitter = CharacterTextSplitter()\n",
        "\n",
        "# Semantic Chunker\n",
        "#semantic_chunker = SemanticChunker()\n",
        "\n",
        "# AI21 Semantic Text Splitter\n",
        "#ai21_splitter = AI21SemanticTextSplitter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embedding Phase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/utente/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define documents\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "\n",
        "sentence_transformer_ef = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedding_function = sentence_transformer_ef \n",
        "\n",
        "# Embed and store document splits\n",
        "vectorstore = Chroma.from_documents(documents=all_docs, embedding=embedding_function)\n",
        "\n",
        "\n",
        "# Other embedding models are available in:\n",
        "# - lang_chain_embedding_models\n",
        "# - sentence-transformers\n",
        "# - Kaggle (includes all ML models, not just embedding ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieving Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Along the way, Dyson engineers made a surprising discovery:' metadata={'page': 4, 'source': './50675-01.pdf', 'start_index': 264}\n",
            "page_content='Along the way, Dyson engineers made a surprising discovery:' metadata={'page': 4, 'source': './50675-01.pdf', 'start_index': 264}\n",
            "page_content='Along the way, Dyson engineers made a surprising discovery:' metadata={'page': 4, 'source': './50675-01.pdf', 'start_index': 264}\n",
            "page_content='Along the way, Dyson engineers made a surprising discovery:' metadata={'page': 4, 'source': './50675-01.pdf', 'start_index': 264}\n",
            "page_content='engineer to carry out a repair).\\nDyson service' metadata={'page': 5, 'source': './50675-01.pdf', 'start_index': 2150}\n",
            "page_content='engineer to carry out a repair).\\nDyson service' metadata={'page': 5, 'source': './50675-01.pdf', 'start_index': 2150}\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "\n",
        "retrieved_docs = retriever.invoke(\"what did the dyson engineers discovery?\")\n",
        "\n",
        "len(retrieved_docs)\n",
        "\n",
        "for retrieved_doc in retrieved_docs:\n",
        "    print(retrieved_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FDQYkRvN3d1n",
        "outputId": "caf71266-f0d2-4f23-cace-1f7352fa069e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, without more specific information about the discovery made by Dyson engineers, I am unable to provide an answer."
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Retrieving top k relevant embeddings\n",
        "def retrieve_top_k(vectorstore, query, k=5, method='cosine'):\n",
        "    if method == 'cosine':\n",
        "        return vectorstore.retrieve(query, k=k, method='cosine')\n",
        "    elif method == 'dot_product':\n",
        "        return vectorstore.retrieve(query, k=k, method='dot_product')\n",
        "    elif method == 'euclidean':\n",
        "        return vectorstore.retrieve(query, k=k, method='euclidean')\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported retrieval method\")\n",
        "\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "  (\"human\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Question: {question} Context: {context} Answer:\"),])\n",
        "\n",
        "\n",
        "# Define RAG Chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "   {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "   | prompt\n",
        "   | llm\n",
        "   | StrOutputParser()\n",
        ")\n",
        "\n",
        "user_prompt = \"what did the dyson engineers discovery?\"\n",
        "# Example usage of the RAG chain\n",
        "for chunk in rag_chain.stream(user_prompt):\n",
        "   print(chunk, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "# The prompt is expected to be a dict with keys \"context\" and \"question\".\n",
        "# retriever | format_docs passes the question through the retriever, generating Document objects, and then to format_docs to generate strings;\n",
        "# RunnablePassthrough() passes through the input question unchanged;\n",
        "# llm runs the inference;\n",
        "# StrOutputParser() plucks the string content out of the LLM's output message.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
